# Model configuration for moral decision consistency research
# Add your API keys and model configurations here

# Cloud Models
openai:
  api_key: ${OPENAI_API_KEY}  # Set via environment variable
  models:
    gpt-4o:
      name: gpt-4o
      supports_seed: true
      default_max_tokens: 500
    gpt-4-turbo:
      name: gpt-4-turbo
      supports_seed: true
      default_max_tokens: 500
    gpt-3.5-turbo:
      name: gpt-3.5-turbo
      supports_seed: true
      default_max_tokens: 500

anthropic:
  api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
  models:
    claude-3.5-sonnet:
      name: claude-3-5-sonnet-20241022
      supports_seed: false
      default_max_tokens: 500
    claude-3-opus:
      name: claude-3-opus-20240229
      supports_seed: false
      default_max_tokens: 500
    claude-3-haiku:
      name: claude-3-haiku-20240307
      supports_seed: false
      default_max_tokens: 500

google:
  api_key: ${GOOGLE_API_KEY}  # Set via environment variable
  models:
    gemini-1.5-pro:
      name: gemini-1.5-pro
      supports_seed: false
      default_max_tokens: 500
    gemini-1.5-flash:
      name: gemini-1.5-flash
      supports_seed: false
      default_max_tokens: 500

# Local Models (vLLM)
vllm:
  endpoint: http://localhost:8000/v1/completions
  models:
    llama-3-8b:
      name: meta-llama/Llama-3-8b-instruct
      supports_seed: true
      default_max_tokens: 500
    qwen-2.5-7b:
      name: Qwen/Qwen2.5-7B-Instruct
      supports_seed: true
      default_max_tokens: 500
    mistral-7b:
      name: mistralai/Mistral-7B-Instruct-v0.3
      supports_seed: true
      default_max_tokens: 500

# Local Models (Ollama)
ollama:
  endpoint: http://localhost:11434/api/generate
  models:
    llama3:
      name: llama3
      supports_seed: true
      default_max_tokens: 500
    qwen2:
      name: qwen2
      supports_seed: true
      default_max_tokens: 500

# Mock model for testing
mock:
  models:
    test-model:
      name: test-model
      supports_seed: true
      default_max_tokens: 500
