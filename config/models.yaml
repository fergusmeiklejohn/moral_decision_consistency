# Model configuration for moral decision consistency research
# Add your API keys and model configurations here

# Cloud Models
openai:
  api_key: ${OPENAI_API_KEY}  # Set via environment variable
  models:
    gpt-5.1:
      name: gpt-5.1
      supports_seed: true
      default_max_tokens: 8192

anthropic:
  api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
  models:
    claude-sonnet-4-5:
      name: claude-sonnet-4-5
      supports_seed: false
      default_max_tokens: 8192

google:
  api_key: ${GOOGLE_API_KEY}  # Set via environment variable
  models:
    gemini-3-pro-preview:
      name: gemini-3-pro-preview
      supports_seed: false
      default_max_tokens: 8192

# Local Models (vLLM)
# vllm:
#   endpoint: http://localhost:8000/v1/completions
#   models:
#     llama-3-8b:
#       name: meta-llama/Llama-3-8b-instruct
#       supports_seed: true
#       default_max_tokens: 8192
#     qwen-2.5-7b:
#       name: Qwen/Qwen2.5-7B-Instruct
#       supports_seed: true
#       default_max_tokens: 8192
#     mistral-7b:
#       name: mistralai/Mistral-7B-Instruct-v0.3
#       supports_seed: true
#       default_max_tokens: 8192

# Local Models (Ollama)
ollama:
  endpoint: http://localhost:11434/api/generate
  models:
    gpt-oss:
      name: gpt-oss:latest  # Default local model (20B tag from Ollama library)
      supports_seed: true
      default_max_tokens: 8192
    qwen3:
      name: qwen3:latest  # Lightweight local option (8B tag from Ollama library)
      supports_seed: true
      default_max_tokens: 8192

# Mock model for testing
mock:
  models:
    mock:
      name: mock
      supports_seed: true
      default_max_tokens: 8192
    test-model:
      name: test-model
      supports_seed: true
      default_max_tokens: 8192
