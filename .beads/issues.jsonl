{"id":"moral_decision_consistency-0jr","content_hash":"b7179835d27d8e8012f00d42199613529bbc282dedf5eeb856ee341260578f17","title":"Align cloud model names and configurable max_tokens","description":"","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-24T11:34:17.27623+08:00","updated_at":"2025-11-24T11:34:24.076474+08:00","closed_at":"2025-11-24T11:34:24.076474+08:00","source_repo":"."}
{"id":"moral_decision_consistency-1cb","content_hash":"94e184123e24489dd9466895ac9e6332ad487b515112c4338febd5f470126116","title":"Stand up pytest scaffolding and fixtures for deterministic testing","description":"Priority: High\nArea: Testing infrastructure\n\nProblem:\nDECISIONS.md now commits us to unit and integration tests with 80% coverage, but the repo has no tests/, fixtures, or pytest configuration. Running pytest today won't exercise anything and would default to hitting real data paths and providers.\n\nScope:\n- tests/ (new) for shared fixtures\n- pytest.ini / pyproject coverage config\n- Minimal fixture assets for dilemmas/configs\n\nTasks:\n- Add pytest configuration that sets testpaths, pythonpath (src/), and default coverage collection (cov=src,scripts with term-missing). Register markers for unit vs integration and keep coverage thresholds configurable (we'll raise to 80% once tests land).\n- Create tests/conftest.py with fixtures for: temporary results directory, a minimal dilemmas.json fixture, and small ExperimentConfig/ExperimentRun factories so tests do not touch data/results or real providers.\n- Add helper/fixture to stub expensive dependencies (e.g., patch MetricsCalculator._get_similarity_model to a lightweight dummy) so tests stay offline and fast.\n- Document the happy-path command (pytest) in CONTRIBUTING notes or README testing section if one exists.\n\nAcceptance Criteria:\n- Command \"pytest -q\" runs using the new config without touching external services or writing to the real data/results tree.\n- Fixtures allow constructing ExperimentConfig/ExperimentRun objects against temporary files and the mock provider without duplicating boilerplate per test.\n- Coverage is collected by default (cov reporting enabled) and markers for unit/integration are recognized.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-20T16:22:20.09989+08:00","updated_at":"2025-11-20T16:32:41.160632+08:00","closed_at":"2025-11-20T16:32:41.160632+08:00","source_repo":"."}
{"id":"moral_decision_consistency-1g8","content_hash":"e1db42ad1a8a8d500fd2d97909756983b52d62906609e310f119500d82f35c79","title":"Extend verify_setup.py to check local model connectivity","description":"Priority: Medium\nArea: Setup verification, local models\n\nProblem:\nverify_setup.py currently checks imports, configs, and mock providers but never exercises Ollama/Qwen, so local misconfiguration only shows up during experiments.\n\nScope:\n- scripts/verify_setup.py\n- config/models.yaml\n\nTasks:\n- Load models config and detect configured ollama/vllm entries.\n- When the Qwen3 ollama model exists, attempt a light connectivity check (e.g., provider.validate_connection() or short generate).\n- Emit success/warning messages like \"✓ Ollama/Qwen3 reachable\" or \"⚠ Ollama/Qwen3 not reachable; is Ollama running?\" without failing the entire script.\n\nAcceptance:\n- verify_setup.py reports a successful connectivity check when Ollama + Qwen3 are running.\n- When Ollama is down or the model is missing, the script prints a clear warning while other checks still complete.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-19T14:37:32.642481+08:00","updated_at":"2025-11-19T16:13:32.631483+08:00","closed_at":"2025-11-19T16:13:32.631483+08:00","source_repo":"."}
{"id":"moral_decision_consistency-6qg","content_hash":"8fd09b2312bcc86b8d9a6c2e902f31f821da9b59f0071f8badac46aadadfb656","title":"Epic: moral framework inference","description":"Implement framework labeling for model reasoning. Phases: 1) heuristic classifier + summaries; 2) optional LLM/embedding labeling with caching; 3) integrate framework metrics into consistency/cost analysis.","status":"open","priority":2,"issue_type":"epic","created_at":"2025-11-26T15:50:41.822049+08:00","updated_at":"2025-11-26T15:50:41.822049+08:00","source_repo":"."}
{"id":"moral_decision_consistency-6qg.1","content_hash":"62834d0161b9ca0733f27a39aec0748655c35c03d67a90d2244f188fbe236dd8","title":"Phase 1: heuristic framework classifier and reporting","description":"Implement a rule/keyword framework classifier over reasoning text; add summaries per model/dilemma/temp; persist labels (JSONL) and expose via analyze script flag; evaluate with small manual seed set if available.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-26T15:50:51.887201+08:00","updated_at":"2025-11-27T16:09:35.430316+08:00","closed_at":"2025-11-27T16:09:35.430324+08:00","source_repo":"."}
{"id":"moral_decision_consistency-6qg.2","content_hash":"afa6186d7b184037bb8f53640a22155a6cfda79a2d530ccd7f0fac4d0c8c2fd8","title":"Phase 2: higher-fidelity labeling (LLM/embeddings) with caching","description":"Add optional LLM-as-judge or embedding+linear probe labeling for frameworks; cache labels by run_id; allow offline/cheap heuristic fallback; measure quality against seed labels.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-26T15:51:02.197433+08:00","updated_at":"2025-11-27T16:09:47.769095+08:00","closed_at":"2025-11-27T16:09:47.769097+08:00","source_repo":"."}
{"id":"moral_decision_consistency-6qg.3","content_hash":"f4cebc2ff2f6aac7727c684228ee08d15747e23ef6002edd336bafd8c90ff27d","title":"Phase 3: integrate framework metrics into analyses","description":"Wire framework labels into analysis outputs: framework flip rates, alignment with choices, cross-model framework agreement, and saved reports/plots; ensure CLI flag produces summaries and caches labels.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-26T15:51:11.533541+08:00","updated_at":"2025-11-27T16:09:55.96678+08:00","closed_at":"2025-11-27T16:09:55.966782+08:00","source_repo":"."}
{"id":"moral_decision_consistency-6qg.4","content_hash":"82c868c207ed743de8c06f10e247cca3333832dada8c6f3824447f32c3015e2c","title":"Framework analysis export per-run (embedding default)","description":"Default framework classification to embedding mode (sentence-transformers), persist per-run analysis JSONL with required fields (run_id, dilemma_id, perturbation_type, position_order, temperature, parsed_choice, reasoning, framework label + confidence, model metadata).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-27T16:35:56.745691+08:00","updated_at":"2025-11-27T16:37:53.988913+08:00","closed_at":"2025-11-27T16:37:53.988914+08:00","source_repo":"."}
{"id":"moral_decision_consistency-782","content_hash":"34347275e544c42f68b0b397c2a58f28e28be39583f80bd67481969a8c08f591","title":"verify_setup reports success even when local models unreachable","description":"","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-26T11:50:24.697537+08:00","updated_at":"2025-11-26T11:52:16.780885+08:00","closed_at":"2025-11-26T11:52:16.780885+08:00","source_repo":"."}
{"id":"moral_decision_consistency-90x","content_hash":"54b7bc3fbc09137a874c707a0fce0895946ffed4cfa2a13d0d3acc306d245ece","title":"Enable config-driven provider mapping so Ollama/Qwen3 can be used in experiments","description":"Priority: High\nArea: Provider selection, configuration\n\nProblem:\nget_provider_from_model_name in src/models/__init__.py forces qwen -\u003e vllm so models defined under config/models.yaml \u003e ollama can never be instantiated. Local Qwen3 via Ollama stays unusable even when configured.\n\nScope:\n- src/models/__init__.py\n- src/experiments/phase1_consistency.py\n- src/experiments/phase2_perturbation.py\n- src/config/loader.py\n\nTasks:\n- Add helper that loads config/models.yaml via ConfigLoader and returns the provider section that declares the requested model.\n- Update Phase 1 and Phase 2 runners to call helper and fall back to heuristic only if model missing from config.\n- Keep provider init errors informative when a model is absent from config.\n\nAcceptance:\n- Adding qwen3-14b-q5 under ollama.models and referencing it in pilot.models instantiates OllamaProvider instead of LocalVLLMProvider.\n- python scripts/run_experiment.py --phase pilot with models [\"qwen3-14b-q5\"] completes provider init and logs provider ollama.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-19T14:36:47.275922+08:00","updated_at":"2025-11-19T15:14:01.249848+08:00","closed_at":"2025-11-19T15:14:01.249848+08:00","source_repo":"."}
{"id":"moral_decision_consistency-9js","content_hash":"c87e56dbdac2cea8f666527147596d98a33394d411526a1fdad567f6578c1b2d","title":"Document the pilot workflow with Qwen3 14B Q5 via Ollama","description":"Priority: Medium\nArea: Documentation (SETUP.md, README.md)\n\nProblem:\nSETUP.md only references llama3 for Ollama examples and there is no explicit walkthrough for the agreed Qwen3 local pilot, leaving contributors unsure how to run it end-to-end.\n\nScope:\n- SETUP.md\n- README.md\n\nTasks:\n- Add a \"Pilot with Qwen3 14B Q5 via Ollama\" subsection detailing pulling the model, confirming config entries, running verify_setup.py, and executing the pilot (direct config or --models override once available).\n- Optionally add a README pointer in the Local Models / Quick start sections.\n\nAcceptance:\n- Following the new SETUP.md subsection from a clean machine plus Ollama is sufficient to run the local pilot without code edits.\n- README references the new workflow so local-first plan is obvious.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-19T14:37:43.703823+08:00","updated_at":"2025-11-19T19:21:08.42091+08:00","closed_at":"2025-11-19T19:21:08.42091+08:00","source_repo":"."}
{"id":"moral_decision_consistency-b7q","content_hash":"30c275f91d3e515a5142d5cbdb8372de77c3dce2f2acbc5bc2f71b47da45c047","title":"Fix Phase2 runner syntax error during run_experiment","description":"","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-26T12:59:17.22249+08:00","updated_at":"2025-11-26T12:59:29.417239+08:00","closed_at":"2025-11-26T12:59:29.417239+08:00","source_repo":"."}
{"id":"moral_decision_consistency-c87","content_hash":"680b82cea85557fe8dd5676dfe68f68a534b15e803ef4eb150c27ceee4c2080c","title":"Ollama endpoints return 404 due to tag mismatch","description":"","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-11-26T12:04:13.035948+08:00","updated_at":"2025-11-26T12:06:19.929368+08:00","closed_at":"2025-11-26T12:06:19.929368+08:00","source_repo":"."}
{"id":"moral_decision_consistency-d58","content_hash":"55cf112065e22974b0f44eb2ab6306538ca678c43b21837debaddcd3ecb51a02","title":"Add mock-backed integration test for run_experiment + analysis","description":"Priority: High\nArea: Testing (integration)\n\nProblem:\nThere is no automated end-to-end check that the experiment runner and analyzer still work together using the mock provider. DECISIONS.md calls for running a mock test before the pilot, but regressions in Phase1Runner/run_experiment.py/analyze_results.py would only show up during a real run. Rate limiting and storage defaults also make it easy to accidentally write into data/results during tests.\n\nScope:\n- scripts/run_experiment.py and src/experiments/phase1_consistency.py\n- scripts/analyze_results.py\n- src/data/storage.py interactions (using temp directories)\n- tests marked integration\n\nTasks:\n- Build a minimal experiment fixture (1 mock model, 1 dilemma, 1 temperature, small num_runs) and use a temporary config directory/dilemmas.json so the test is fully isolated from repo data.\n- Instantiate Phase1Runner with the mock provider, temp storage, and the fixture loaders; monkeypatch time.sleep/rate limiting so the test runs quickly while still exercising the run loop and backup path.\n- Assert the run produces the expected number of ExperimentRun records and summary stats in the temp results directory (no writes to data/results/). Verify backups/analysis files are created where expected.\n- Invoke analyze_experiment (or its helper) on the produced experiment while stubbing MetricsCalculator._get_similarity_model to avoid downloads, and assert key metrics (CCR, refusal/error rates, flip counts) are computed without exceptions.\n- Wire the test under a marker (integration) so it can be selectively run via `pytest -m integration` and included in the default suite if runtime remains short.\n\nAcceptance Criteria:\n- `pytest -m integration` executes the mock end-to-end run in a temporary location without network calls or touching real data/results, and completes in under ~15 seconds on a laptop.\n- The integration test verifies both runner and analyzer complete successfully and produce expected run counts/summary/analysis artifacts.\n- Rate limiting sleeps are effectively neutralized during the test so it remains fast while still covering the run loop logic.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-20T16:23:17.762848+08:00","updated_at":"2025-11-20T17:19:25.907769+08:00","closed_at":"2025-11-20T17:19:25.907769+08:00","source_repo":".","dependencies":[{"issue_id":"moral_decision_consistency-d58","depends_on_id":"moral_decision_consistency-1cb","type":"discovered-from","created_at":"2025-11-20T16:23:17.763722+08:00","created_by":"fergusmeiklejohn"}]}
{"id":"moral_decision_consistency-pkx","content_hash":"241d0d3cea3e18dfa60ecc2d0786c84b316900ecc772f98f61afb05b5f61e6bb","title":"Add Qwen3 14B Q5 Ollama config and switch the pilot to the local model","description":"Priority: High\nArea: Configuration\n\nProblem:\nDECISIONS.md mandates the pilot use Qwen3 14B Q5 via Ollama, but config/models.yaml still has only generic local placeholders and config/experiment.yaml keeps cloud pilots.\n\nScope:\n- config/models.yaml\n- config/experiment.yaml\n\nTasks:\n- Add a concrete qwen3-14b-q5 entry under ollama.models with appropriate metadata (name, supports_seed, default_max_tokens, etc.).\n- Update pilot.models to reference only the new Qwen model so the local-first pilot aligns with decisions.\n- Optionally annotate how to re-add clouds later.\n\nAcceptance:\n- With Ollama running and Qwen3 pulled, python scripts/run_experiment.py --phase pilot succeeds using only the local Qwen model.\n- Stored experiment config lists only the Qwen3 entry in models.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-19T14:37:00.776851+08:00","updated_at":"2025-11-19T15:23:40.73309+08:00","closed_at":"2025-11-19T15:23:40.73309+08:00","source_repo":"."}
{"id":"moral_decision_consistency-s3f","content_hash":"8a77d3516616b2c705a43df3d87863489c11a66c6b31113e995156d87e9daa46","title":"Add requests to dependencies and setup checks so local providers import cleanly","description":"Priority: High\nArea: Dependencies, setup verification\n\nProblem:\nsrc/models/local_model.py imports requests for LocalVLLMProvider and OllamaProvider but requirements.txt omits it and scripts/verify_setup.py never checks for it, so fresh installs fail when local providers load.\n\nScope:\n- requirements.txt\n- scripts/verify_setup.py\n- src/models/local_model.py\n\nTasks:\n- Add requests\u003e=2.0.0 to requirements.txt under core dependencies.\n- Extend verify_setup.py verify_imports() to include (\"requests\", \"requests\") so missing requests surfaces.\n\nAcceptance:\n- After pip install -r requirements.txt, python scripts/verify_setup.py reports requests present.\n- Importing LocalVLLMProvider or OllamaProvider works without ImportError.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-19T14:37:11.426828+08:00","updated_at":"2025-11-19T15:31:34.854435+08:00","closed_at":"2025-11-19T15:31:34.854435+08:00","source_repo":"."}
{"id":"moral_decision_consistency-thz","content_hash":"dccc24329c1d4cd32b5c692dd1936d8328672483b4cadaa557c37399dbccfed1","title":"Implement --models CLI override in run_experiment.py","description":"Priority: Medium-High\nArea: CLI script\n\nProblem:\nREADME advertises --models overrides and scripts/run_experiment.py defines the argument, but it never affects the run and --config is unimplemented, forcing config edits for every pilot tweak.\n\nScope:\n- scripts/run_experiment.py\n- README.md\n\nTasks:\n- Load experiment config via ConfigLoader in run_experiment.py, override models when --models is provided, and pass the resulting ExperimentConfig into runners instead of letting them reload.\n- Either implement basic --config support (alternate YAML path) or remove the flag/docs for now.\n- Update README examples to match the behavior.\n\nAcceptance:\n- python scripts/run_experiment.py --phase pilot --models qwen3-14b-q5 runs using only the provided model list.\n- CLI help and README usage reflect the implemented flags.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-19T14:37:22.094766+08:00","updated_at":"2025-11-19T15:42:27.415455+08:00","closed_at":"2025-11-19T15:42:27.415455+08:00","source_repo":"."}
{"id":"moral_decision_consistency-ts4","content_hash":"9cf5a1cf20716bcd7f56264532f9cac6720ffcd4519561e6847302d1cd044ee8","title":"Implement Synthetic Internal Step Error (Type C) perturbation","description":"Priority: Backlog\nArea: Phase II experimentation, reasoning graphs, metrics\nReference: DECISIONS.md Synthetic Error Injection section\n\nProblem:\nPhase II calls for Perturbation Type C (Synthetic Internal Step Error) with structured reasoning steps, targeted error injection, repair prompts, and new metrics, but none of this exists yet.\n\nScope:\n- src/experiments/phase2_perturbation.py\n- src/data/schemas.py\n- src/analysis/metrics.py\n\nMinimal Spec:\n- Pass 1: prompt for 3-6 named steps with dependency edges and one-line claims (structured JSON).\n- Error injection: apply transforms (probability_swap, sign_flip, culpability_misattribution, premise_drop, numerical_offset) to a chosen step.\n- Pass 2: repair prompt instructing the model to find the mistake in Step k, minimally repair downstream steps, and re-decide outcome.\n- Metrics: localization accuracy, repair success, minimality score, counterfactual coherence, explanation-perturbation alignment.\n\nTasks:\n- Add Type C prompting + error injection pipeline to Phase2Runner and ReasoningGraph handling.\n- Log the second-pass repair data needed for metrics.\n- Extend metrics module to compute the Phase II measures for Type C runs.\n\nAcceptance:\n- Phase II can run with perturbation_types including synthetic_error to produce structured reasoning graphs, injected errors, and repair passes.\n- Analysis reports localization, repair success, minimality, and coherence metrics for those runs.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-11-19T14:37:56.035792+08:00","updated_at":"2025-11-19T19:30:39.541818+08:00","closed_at":"2025-11-19T19:30:39.541818+08:00","source_repo":"."}
{"id":"moral_decision_consistency-utx","content_hash":"5f4d9ed54952e7ffee9b13f8a44ab7651f238a586cbf35a9b1b40fb93ff7a008","title":"Pilot run: mock provider resolution and .env loading failing","description":"","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-11-24T10:59:01.342392+08:00","updated_at":"2025-11-24T11:07:00.092737+08:00","closed_at":"2025-11-24T11:07:00.092737+08:00","source_repo":"."}
{"id":"moral_decision_consistency-ve5","content_hash":"152fcdccd34dab90a980ad9ae2630f67e20f24d964bfac5846ab5ba30975275c","title":"Document uv-based environment setup and removal of requirements.txt","description":"Priority: Medium\nArea: Tooling docs\n\nProblem:\nProject now uses uv + pyproject.toml (requirements.txt removed) but documentation still instructs pip + requirements.txt. Agents/README/SETUP need updated guidance so contributors use uv-managed venvs and know how to install/run tests.\n\nScope:\n- Agents.md (agent operating instructions)\n- README.md quickstart\n- SETUP.md install section\n\nTasks:\n- Clarify that dependency management is via uv with the checked-in uv.lock/pyproject.toml and that requirements.txt is removed.\n- Provide install steps using uv (e.g., uv venv \u0026\u0026 uv sync) and note how to activate the venv.\n- Update testing instructions to reflect pytest availability through uv-managed env.\n- Retire/replace pip -r references and add a brief note about not using requirements.txt to avoid drift.\n\nAcceptance Criteria:\n- Docs give a consistent uv-first install flow and mention the removal of requirements.txt.\n- New contributors following the docs can create/activate the uv venv and run pytest successfully.\n- No remaining pip -r references for this repo.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-11-20T17:09:57.113988+08:00","updated_at":"2025-11-20T17:13:10.805433+08:00","closed_at":"2025-11-20T17:13:10.805433+08:00","source_repo":"."}
{"id":"moral_decision_consistency-ze1","content_hash":"12c210628107fe446fab669756427ca2e586caf119770a78c7de2ccdd33ee4f1","title":"Write unit tests for core modules and enforce 80% coverage","description":"Priority: High\nArea: Testing (unit coverage)\n\nProblem:\nThere are zero automated tests covering core logic (config loading, dilemma prompts, storage, schema parsing, metrics). DECISIONS.md sets an 80% coverage goal for core modules. Without targeted unit tests we risk regressions in config resolution, prompt formatting, data persistence, and metric calculations before the pilot or Phase I runs.\n\nScope:\n- src/config/loader.py\n- src/dilemmas/loader.py\n- src/data/schemas.py and src/data/storage.py\n- src/analysis/metrics.py (with SentenceTransformer mocked/stubbed)\n- src/models/base.py parsing helpers where relevant\n\nTasks:\n- Add unit tests for ConfigLoader: env var substitution, missing experiment/model errors, provider-level fields propagating into model configs.\n- Add tests for DilemmaLoader: prompt generation (normal/reversed), perturbation variant lookup/validation, and error paths for missing variants.\n- Cover schema parsing helpers (Choice parsing in ModelResponse.field_validator, ExperimentConfig defaults) and ExperimentStorage save/load/get_experiment_summary/backups using tmp paths so repo data/results is untouched.\n- Add tests for MetricsCalculator: CCR/refusal/error/flip calculations, perturbation sensitivity for relevant vs irrelevant runs, cross-model agreement, MSS quadrant classification, and Type C helpers (_collect_allowed_steps + calculate_type_c_metrics) built from synthetic ReasoningGraph/TypeCRecord objects. Stub _get_similarity_model to a lightweight encoder to avoid downloading models.\n- After tests are in place, raise coverage enforcement to 80% in pytest config (cov-fail-under=80) for the covered modules.\n\nAcceptance Criteria:\n- `pytest --maxfail=1` passes offline with new unit tests and does not touch external providers or the real data/results directory.\n- Coverage report (cov=src,scripts) shows at least 80% for the targeted modules and enforces the threshold via pytest configuration.\n- Tests include both happy-path and error-path cases for config/loader, dilemmas, storage, schema parsing, metrics, and Type C utilities.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-11-20T16:22:56.228112+08:00","updated_at":"2025-11-20T17:19:53.546523+08:00","closed_at":"2025-11-20T17:19:53.546523+08:00","source_repo":".","dependencies":[{"issue_id":"moral_decision_consistency-ze1","depends_on_id":"moral_decision_consistency-1cb","type":"discovered-from","created_at":"2025-11-20T16:22:56.229407+08:00","created_by":"fergusmeiklejohn"}]}
